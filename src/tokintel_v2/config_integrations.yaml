# TokIntel v2 Configuration with Integrations
# This file contains all configuration settings for TokIntel including new integrations

# Basic settings
model: "gpt-4"
language: "it"

# Analysis keywords
keywords:
  - "motivazione"
  - "hook"
  - "viral"
  - "emozione"
  - "engagement"
  - "trend"

# Analysis weights
weights:
  keywords: 1.5
  speech_density: 1.0
  ocr: 1.2
  engagement: 1.0
  viral_potential: 1.3

# Output settings
export_format: ["csv", "json"]
output_folder: "output/"

# LLM configuration
llm_config:
  model: "gpt-4"
  endpoint: null  # Set to local endpoint if using local LLM
  api_key: null   # Set your OpenAI API key here
  timeout: 30
  max_retries: 3
  temperature: 0.7
  max_tokens: 500

# Processing settings
frame_extraction_interval: 30
max_video_duration: 300

# Logging settings
log_level: "INFO"
log_file: null  # Set to file path for file logging

# Whisper Integration
whisper:
  use_local: true
  use_api: false
  model_size: "base"  # tiny, base, small, medium, large
  device: "auto"  # auto, cpu, cuda
  batch_size: 16
  max_retries: 3
  api_key: null  # OpenAI API key for Whisper API
  supported_formats:
    audio: [".wav", ".mp3", ".m4a", ".flac"]
    video: [".mp4", ".avi", ".mov", ".mkv", ".webm"]

# LLM Multi-Backend Configuration
llm:
  preferred_backend: "lmstudio"  # lmstudio, ollama, openai
  fallback_strategy: "auto"  # auto, manual, none
  
  # LM Studio Integration
  lmstudio:
    base_url: "http://localhost:1234/v1/chat/completions"
    model: "mistral"
    timeout: 30
    max_retries: 3
    temperature: 0.7
    max_tokens: 2048
    top_p: 1.0
    frequency_penalty: 0.0
    presence_penalty: 0.0
  
  # Ollama Integration
  ollama:
    base_url: "http://localhost:11434"
    model: "mistral"
    timeout: 30
    max_retries: 3
    retry_delay: 1
    temperature: 0.7
    max_tokens: 500
    available_models:
      - "mistral"
      - "llama2:7b"
      - "llama2:13b"
      - "codellama:7b"
      - "neural-chat"
  
  # OpenAI Integration (future)
  openai:
    api_key: "${OPENAI_API_KEY}"
    model: "gpt-3.5-turbo"
    timeout: 30
    max_tokens: 2048

# HuggingFace Integration
huggingface:
  device: "auto"  # auto, cpu, cuda
  use_gpu: true
  max_length: 512
  batch_size: 1
  cache_dir: null
  models:
    summarization: "facebook/bart-large-cnn"
    classification: "distilbert-base-uncased"
    sentiment: "cardiffnlp/twitter-roberta-base-sentiment-latest"
    ner: "dbmdz/bert-large-cased-finetuned-conll03-english"
    translation: "Helsinki-NLP/opus-mt-en-it"
    question_answering: "distilbert-base-cased-distilled-squad"

# Integration Pipeline
pipeline:
  # Audio/Video processing
  audio_extraction: true
  transcription: true
  language_detection: true
  
  # Text processing
  summarization: true
  sentiment_analysis: true
  keyword_extraction: true
  entity_recognition: true
  
  # LLM processing
  local_llm: true
  cloud_llm: true
  fallback_strategy: "local_first"  # local_first, cloud_first, hybrid
  
  # Output generation
  generate_summary: true
  generate_insights: true
  generate_recommendations: true

# Performance settings
performance:
  parallel_processing: true
  max_workers: 4
  memory_limit: "4GB"
  gpu_memory_fraction: 0.8
  
# Quality settings
quality:
  transcription_confidence_threshold: 0.8
  sentiment_confidence_threshold: 0.7
  entity_confidence_threshold: 0.8
  summary_min_length: 30
  summary_max_length: 200

# Monitoring and logging
monitoring:
  enable_metrics: true
  log_processing_times: true
  log_errors: true
  save_intermediate_results: true
  performance_benchmarking: true 